import unittest

from opustrainer.tokenizers import make_tokenizer, make_detokenizer

@unittest.skip("requires installing pyicu")
class TestTokenizers(unittest.TestCase):

  def test_tokenize_detokenize_icu_en(self):
    """
    Tests lossless text reconstruction by the ICU tokenizer for English.
    Requires installation with the steps specified in https://pypi.org/project/PyICU/
    """
    tokenizer = make_tokenizer('icu:en')
    detokenizer = make_detokenizer('icu:en')
    text = 'â€œThis is,â€ a simple test statement ğŸ¤£.'

    tokens, _ = tokenizer.tokenize(text)
    detokenized, _ = detokenizer.detokenize(tokens)

    self.assertEqual(text, detokenized)
    self.assertEqual("â€œ This â– is , â€ â– a â– simple â– test â– statement â– ğŸ¤£.", " ".join(tokens))


  def test_tokenize_detokenize_icu_zh(self):
    """
    Tests lossless text reconstruction by the ICU tokenizer for Chinese.
    Requires installation with the steps specified in https://pypi.org/project/PyICU/
    """
    tokenizer = make_tokenizer('icu:zh')
    detokenizer = make_detokenizer('icu:zh')
    text = 'è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯­å¥ ğŸ¤£ ã€‚'

    tokens, _ = tokenizer.tokenize(text)
    detokenized, _ = detokenizer.detokenize(tokens)

    self.assertEqual(text, detokenized)
    self.assertEqual("è¿™ æ˜¯ ä¸€ä¸ª ç®€å• çš„ æµ‹è¯• è¯­ å¥ â– ğŸ¤£â– ã€‚", " ".join(tokens))
